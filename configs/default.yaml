# Default configuration for SWELM experiments

# Model configuration
model:
  encoder: "xlm-roberta-base"
  llm: "meta-llama/Meta-Llama-3-8B"
  load_in_8bit: false

# SWELM parameters
swelm:
  alpha: 0.5  # Weight for semantic distance
  beta: 1.0   # Weight for representation quality
  sampling_strategy: "proportional"
  temperature: 1.0
  min_weight: 0.1

# Data configuration
data:
  languages:
    - en
    - es
    - fr
    - de
    - zh
    - ar
    - hi
  batch_size: 32
  max_length: 512

# Training configuration
training:
  seed: 42
  num_epochs: 3
  learning_rate: 5e-5
  warmup_steps: 500
  gradient_accumulation_steps: 4

# Evaluation configuration
evaluation:
  metrics:
    - exact_match
    - f1
    - bleu
  output_dir: "results"

# Baseline configuration
baselines:
  temperature: 1.5
  diversity_penalty: 0.5

# Logging
logging:
  level: "INFO"
  log_file: null
